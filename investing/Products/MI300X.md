---
aliases:
  - MI300X
  - Instinct MI300X
  - AMD MI300X
tags:
  - product
  - semiconductor
  - ai-accelerator
parent_actor: "[[AMD]]"
parent_concept: "[[AI accelerators]]"
---

# MI300X

[[AMD]]'s flagship AI accelerator. Primary competitor to [[NVIDIA]] [[H100]]/[[H200]]. 192GB HBM3 — more memory than H100. CDNA 3 architecture. Launched December 2023.

## Quick stats

| Metric | Value |
|--------|-------|
| Architecture | CDNA 3 |
| Process | TSMC 5nm/6nm (chiplet) |
| Transistors | 153B |
| HBM3 memory | **192GB** (vs H100's 80GB) |
| Memory bandwidth | **5.3 TB/s** (vs H100's 3.35 TB/s) |
| TDP | 750W |
| FP16 performance | 1,307 TFLOPS |
| FP8 performance | 2,614 TFLOPS |
| List price | ~$15,000-20,000 |
| Launched | December 2023 |

---

## Version history

| Model | Release | Key changes |
|-------|---------|-------------|
| MI250X | 2021 | CDNA 2, 128GB HBM2e |
| MI250 | 2021 | Lower-spec variant |
| **MI300X** | Dec 2023 | CDNA 3, 192GB HBM3, chiplet design |
| MI300A | Dec 2023 | APU variant (CPU+GPU integrated) |
| **MI325X** | Q4 2024 | 256GB HBM3e, 6 TB/s |
| MI350 | 2025 | CDNA 4 architecture |
| MI400 | 2026 | Next-gen (announced) |

---

## vs NVIDIA H100

| Spec | MI300X | H100 SXM | Advantage |
|------|--------|----------|-----------|
| HBM capacity | **192GB** | 80GB | MI300X (2.4x) |
| Bandwidth | **5.3 TB/s** | 3.35 TB/s | MI300X (1.6x) |
| FP8 | 2,614 TFLOPS | 3,958 TFLOPS | H100 |
| TDP | 750W | 700W | H100 (slightly) |
| Price | ~$15-20K | ~$30K | MI300X |
| Software | ROCm | CUDA | H100 (ecosystem) |

**AMD's pitch:** More memory, more bandwidth, lower price. Loses on raw compute and software ecosystem.

---

## Architecture

| Feature | Description |
|---------|-------------|
| Chiplet design | 8 XCDs + 4 IODs on package |
| Infinity Fabric | High-bandwidth die-to-die interconnect |
| Unified memory | CPU can access GPU HBM directly |
| ROCm | AMD's CUDA alternative |
| OCP OAM | Open accelerator module form factor |

---

## Software ecosystem (ROCm)

| Aspect | Status |
|--------|--------|
| PyTorch | Supported |
| TensorFlow | Supported |
| JAX | Supported |
| vLLM | Supported |
| Flash Attention | Ported |
| Triton | Supported |
| CUDA porting | Via HIP translation |

**The gap:** ROCm works but CUDA ecosystem is larger. Enterprise customers often cite software as reason to stay with NVIDIA.

---

## Customer wins

| Customer | Deployment |
|----------|------------|
| [[Microsoft]] | Azure MI300X instances |
| [[Meta]] | Evaluation clusters |
| [[Oracle]] | OCI deployment |
| [[IBM]] | Cloud instances |
| [[Dell]] | PowerEdge servers |
| [[HPE]] | ProLiant servers |

Microsoft Azure was the first major cloud to offer MI300X instances.

---

## AMD AI revenue

| Period | AI data center revenue |
|--------|----------------------|
| Q4 2023 | ~$400M |
| Q1 2024 | ~$600M |
| Q2 2024 | ~$1B |
| 2024 guidance | $4.5B+ |
| 2025 target | $5B+ |

MI300X driving AMD's AI revenue ramp, but still small vs NVIDIA's $40B+ data center run rate.

---

## Competitive position

| vs | MI300X advantage | MI300X disadvantage |
|----|------------------|---------------------|
| [[H100]] | Memory, price | Compute, CUDA ecosystem |
| [[H200]] | Price | Memory now closer, CUDA |
| [[B200]] | Price | Everything else |
| [[Gaudi 3]] | Availability | Intel's different approach |

**AMD's strategy:** Win on memory capacity and price for inference workloads. Concede training to NVIDIA.

---

## Related

- [[AMD]] — parent actor
- [[AI accelerators]] — parent concept
- [[H100]] — primary competitor
- [[H200]] — competitor (memory focus)
- [[B200]] — next-gen competitor
- [[Gaudi 3]] — Intel competitor
- [[ROCm]] — AMD's GPU software stack
- [[TSMC]] — foundry partner
- [[HBM]] — key memory technology
