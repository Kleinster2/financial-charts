---
aliases:
  - H200
  - H200 SXM
  - Hopper H200
tags:
  - product
  - semiconductor
  - ai-accelerator
parent_actor: "[[NVIDIA]]"
parent_concept: "[[AI accelerators]]"
---

# H200

[[NVIDIA]]'s Hopper refresh with HBM3e memory. Same architecture as [[H100]], 1.8x memory capacity and 1.4x bandwidth. Optimized for large model inference. Bridge between [[H100]] and [[B200]].

## Quick stats

| Metric | Value |
|--------|-------|
| Architecture | Hopper (same as H100) |
| Process | TSMC 4N |
| Transistors | 80B |
| HBM3e memory | **141GB** (vs H100's 80GB) |
| Memory bandwidth | **4.8 TB/s** (vs H100's 3.35 TB/s) |
| TDP | 700W (SXM) |
| FP8 performance | 3,958 TFLOPS (same as H100) |
| Announced | November 2023 (SC23) |
| Volume | Q2 2024 onwards |

---

## vs H100

| Spec | H100 | H200 | Improvement |
|------|------|------|-------------|
| Architecture | Hopper | Hopper | Same |
| Transistors | 80B | 80B | Same |
| HBM capacity | 80GB HBM3 | **141GB HBM3e** | 1.76x |
| Bandwidth | 3.35 TB/s | **4.8 TB/s** | 1.43x |
| FP8 | 3,958 TFLOPS | 3,958 TFLOPS | Same |
| TDP | 700W | 700W | Same |

**Key insight:** Same compute, more memory. Enables larger batch sizes and bigger models without architectural change.

---

## Why H200 matters

| Use case | Benefit |
|----------|---------|
| LLM inference | Larger KV cache, more concurrent users |
| Long context | 141GB fits bigger context windows |
| Large models | Run 70B+ models more efficiently |
| Upgrade path | Drop-in replacement for H100 systems |

**Inference economics:** Memory-bound workloads (most LLM inference) benefit more from bandwidth than raw FLOPS.

---

## Variants

| Variant | Form factor | Notes |
|---------|-------------|-------|
| H200 SXM | Server module | NVLink-enabled, 700W |
| H200 NVL | Dual-GPU bridge | High-memory inference |

No PCIe variant announced — SXM-only for data center deployment.

---

## China variant

| Chip | Spec change | Purpose |
|------|-------------|---------|
| H20 | Significantly reduced specs | Export-compliant for China |

H20 is the China-legal variant, much more constrained than H200.

---

## Demand signals

| Customer | Status |
|----------|--------|
| [[Meta]] | Major deployment |
| [[Microsoft]] | Azure instances |
| [[Amazon]] | AWS instances |
| [[Google]] | GCP instances |
| [[Oracle]] | OCI deployment |
| [[ByteDance]] | $14B order (H200s) |

Cloud providers ramped H200 instances through 2024-2025.

---

## Positioning

```
A100 (2020) → H100 (2022) → H200 (2024) → B200 (2024)
   Ampere        Hopper       Hopper+       Blackwell
                              (memory)      (new arch)
```

H200 extends Hopper life while Blackwell ramps. Useful for customers who want more memory without system redesign.

---

## Related

- [[NVIDIA]] — parent actor
- [[AI accelerators]] — parent concept
- [[H100]] — base architecture
- [[B200]] — successor (Blackwell architecture)
- [[A100]] — predecessor generation
- [[HBM]] — key differentiator (HBM3e vs HBM3)
- [[TSMC]] — foundry partner
