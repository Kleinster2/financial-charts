---
aliases:
  - H100
  - Hopper
  - H100 SXM
  - H100 PCIe
tags:
  - product
  - semiconductor
  - ai-accelerator
parent_actor: "[[NVIDIA]]"
parent_concept: "[[AI accelerators]]"
---

# H100

[[NVIDIA]]'s data center GPU for AI training and inference. Launched March 2022, volume production 2023. The workhorse of the [[AI hyperscalers]] buildout.

## Quick stats

| Metric | Value |
|--------|-------|
| Architecture | Hopper |
| Process | TSMC 4N |
| Transistors | 80B |
| HBM3 memory | 80GB |
| Memory bandwidth | 3.35 TB/s |
| TDP | 700W (SXM), 350W (PCIe) |
| FP8 performance | 3,958 TFLOPS |
| List price | ~$30,000 (varies by config) |
| Launch | March 2022 (announced), 2023 (volume) |

## Variants

| Variant | Form factor | Use case |
|---------|-------------|----------|
| H100 SXM | Server module with NVLink | Large-scale training clusters |
| H100 PCIe | Standard PCIe card | Inference, smaller deployments |
| H100 NVL | Dual-GPU NVLink bridge | High-memory inference |

## Significance

- First GPU with Transformer Engine (FP8 precision for LLMs)
- 4th-gen NVLink (900 GB/s GPU-to-GPU)
- Defined the [[AI infrastructure]] buildout 2023-2024
- Supply-constrained throughout 2023-2024; drove [[NVIDIA]] earnings beats
- Benchmark for AI accelerator competition ([[AMD]] MI300X, [[Intel]] Gaudi)

## Related

- [[NVIDIA]] — parent actor
- [[AI accelerators]] — parent concept
- [[B200]] — successor (Blackwell architecture, 2024)
- [[A100]] — predecessor (Ampere architecture)
- [[HBM]] — key memory technology
- [[TSMC]] — foundry partner
