---
aliases:
  - Kimi
  - Kimi K2
  - Kimi K2.5
  - Moonshot Kimi
tags:
  - product-family
  - ai
  - china
parent_actor: "[[Moonshot AI]]"
parent_concept: "[[Frontier models]]"
---

# Kimi

[[Moonshot AI]]'s long-context LLM. Pioneered 128K context (2023). K2 (Jul 2025) has 1T parameters, 256K context, runs 200-300 sequential tool calls. First context-obsessed AI lab.

## Quick stats

| Metric | Value |
|--------|-------|
| Current version | K2.5 (Jan 2026) |
| Context window | 256K tokens |
| Total parameters | 1 trillion (K2) |
| Active parameters | 32B (MoE) |
| Training cost (K2) | $4.6M |
| Key strength | Long context, agentic |

---

## Version history

| Model | Release | Key changes |
|-------|---------|-------------|
| Kimi | 2023 | First 128K context model |
| Kimi (2M) | Mar 2024 | 2 million character context |
| Context caching | Jul 2024 | Cost reduction for long prompts |
| **Kimi K1.5** | Jan 2025 | Matched o1 (math, coding, multimodal) |
| **Kimi K2** | Jul 2025 | 1T params, 256K context, open source |
| Kimi K2 Thinking | Nov 2025 | Reasoning, 300 tool calls |
| **Kimi K2.5** | Jan 2026 | Multimodal (MoonViT encoder) |

---

## Architecture (K2)

| Aspect | Details |
|--------|---------|
| Total params | 1 trillion |
| Active params | 32B (MoE) |
| Training data | 15.5 trillion tokens |
| Attention | Kimi Delta Attention (KDA) |
| License | Modified MIT (open source) |

KDA reduces memory and improves speed at long context.

---

## Agentic capabilities

| Feature | K2 Thinking |
|---------|-------------|
| Tool calls | 200-300 sequential |
| Context | 256K tokens |
| Autonomy | Extended multi-step tasks |
| Use cases | Research, automation, coding |

---

## Context evolution

| Date | Context size |
|------|--------------|
| 2023 | 128K tokens (first ever) |
| Mar 2024 | 2M characters |
| Jul 2024 | Context caching (cost reduction) |
| Sep 2025 | 256K tokens |

"The journey from 128K to 300 tool calls took 30 months."

---

## Competitive position

| vs | Kimi advantage | Kimi weakness |
|----|---------------|---------------|
| [[GPT]] | Context, open source | Scale, ecosystem |
| [[Claude]] | Context length | Brand, safety research |
| [[DeepSeek-V]] | Agentic focus | DeepSeek efficiency |

Context-first architecture enables unique agentic capabilities.

---

## Roadmap (2026-2027)

| Goal | Details |
|------|---------|
| AGI Layer 2 | Continual self-training |
| Vertical agents | Law, medicine |
| International | Cloud regions expansion |
| Marketplace | Community tool ecosystem |

---

## Related

- [[Moonshot AI]] — parent actor
- [[GPT]] — competitor
- [[Claude]] — competitor
- [[DeepSeek-V]] — competitor
- [[Qwen]] — competitor (China)
- [[Frontier models]] — category
- [[China]] — market
