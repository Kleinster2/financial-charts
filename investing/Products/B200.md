---
aliases:
  - B200
  - Blackwell
  - GB200
  - B200 SXM
tags:
  - product
  - semiconductor
  - ai-accelerator
parent_actor: "[[NVIDIA]]"
parent_concept: "[[AI accelerators]]"
---

# B200

[[NVIDIA]]'s flagship data center GPU for AI training and inference. Blackwell architecture. Successor to [[H100]]. Volume production 2024-2025.

## Quick stats

| Metric | Value |
|--------|-------|
| Architecture | Blackwell |
| Process | TSMC 4NP |
| Transistors | 208B |
| HBM3e memory | 192GB |
| Memory bandwidth | 8 TB/s |
| TDP | 1000W (SXM) |
| FP8 performance | 9 PFLOPS (2x H100) |
| FP4 performance | 18 PFLOPS |
| List price | ~$30,000-40,000 |
| Announced | March 2024 (GTC) |
| Volume | H2 2024 onwards |

---

## Version history

| Variant | Config | Use case |
|---------|--------|----------|
| B200 | Single GPU | Standard deployment |
| **GB200** | 2x B200 + Grace CPU | NVLink-connected superchip |
| GB200 NVL72 | 72 GPUs in rack | Large-scale training |
| B200A | Air-cooled variant | Retrofits existing data centers |

---

## Key improvements over H100

| Spec | H100 | B200 | Improvement |
|------|------|------|-------------|
| Transistors | 80B | 208B | 2.6x |
| HBM capacity | 80GB | 192GB | 2.4x |
| Memory bandwidth | 3.35 TB/s | 8 TB/s | 2.4x |
| FP8 performance | 4 PFLOPS | 9 PFLOPS | 2.25x |
| TDP | 700W | 1000W | 1.4x |

---

## Architecture features

| Feature | Description |
|---------|-------------|
| 2nd-gen Transformer Engine | FP4 support for inference |
| 5th-gen NVLink | 1.8 TB/s GPU-to-GPU |
| Decompression engine | On-chip data decompression |
| RAS engine | Reliability, availability, serviceability |
| Secure AI | Confidential computing |

---

## GB200 superchip

Two B200 GPUs connected to one Grace CPU via NVLink-C2C:

| Spec | Value |
|------|-------|
| GPUs | 2x B200 |
| CPU | 1x Grace (72 Arm cores) |
| Combined memory | 384GB HBM3e + 480GB LPDDR5X |
| Interconnect | NVLink-C2C (900 GB/s) |
| Form factor | Single board |

**GB200 NVL72:** 36 GB200 superchips (72 GPUs) in a single rack with liquid cooling. 720 PFLOPS FP8.

---

## Demand signals

| Customer | Order |
|----------|-------|
| [[Microsoft]] | Major GB200 deployment |
| [[Meta]] | 350K+ H100 equivalent planned |
| [[xAI]] | Colossus cluster (100K+ GPUs) |
| [[Oracle]] | Cloud deployment |
| [[CoreWeave]] | Infrastructure buildout |

Supply constrained through 2025.

---

## Competitive position

| vs | B200 advantage | B200 disadvantage |
|----|----------------|-------------------|
| [[AMD]] MI300X | Performance, software (CUDA) | Price, availability |
| [[Intel]] Gaudi 3 | Performance, ecosystem | Competition emerging |
| [[Google]] TPU v5p | General availability | Google-only |
| Custom ASICs | Flexibility, ecosystem | Efficiency for specific workloads |

---

## Related

- [[NVIDIA]] — parent actor
- [[AI accelerators]] — parent concept
- [[H100]] — predecessor (Hopper architecture)
- [[H200]] — Hopper refresh with HBM3e
- [[HBM]] — key memory technology
- [[TSMC]] — foundry partner
- [[NVLink]] — interconnect technology
- [[Power constraints]] — data center challenge (1000W TDP)
