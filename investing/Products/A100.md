---
aliases:
  - A100
  - Ampere
  - A100 SXM
  - A100 PCIe
tags:
  - product
  - semiconductor
  - ai-accelerator
parent_actor: "[[NVIDIA]]"
parent_concept: "[[AI accelerators]]"
---

# A100

[[NVIDIA]]'s data center GPU that defined the early AI training era. Ampere architecture. Powered GPT-3, DALL-E, and the first wave of foundation models. Predecessor to [[H100]].

## Quick stats

| Metric | Value |
|--------|-------|
| Architecture | Ampere |
| Process | TSMC 7N |
| Transistors | 54B |
| HBM2e memory | 40GB / 80GB |
| Memory bandwidth | 1.6 TB/s (40GB) / 2 TB/s (80GB) |
| TDP | 400W (SXM), 250W (PCIe) |
| FP16 performance | 312 TFLOPS |
| TF32 performance | 156 TFLOPS |
| Announced | May 2020 (GTC) |
| Volume | 2020-2023 peak |

---

## Version history

| Variant | Memory | Notes |
|---------|--------|-------|
| A100 40GB | 40GB HBM2e | Original launch |
| A100 80GB | 80GB HBM2e | Nov 2020, doubled memory |
| A100 PCIe | 40/80GB | Standard PCIe form factor |
| A100 SXM | 40/80GB | NVLink-enabled server module |
| A100X | 80GB | Converged accelerator (networking) |

---

## Architecture features

| Feature | Description |
|---------|-------------|
| 3rd-gen Tensor Cores | TF32, FP64, INT8, BF16 |
| MIG | Multi-Instance GPU (7 partitions) |
| 3rd-gen NVLink | 600 GB/s GPU-to-GPU |
| Structural sparsity | 2:4 sparsity for 2x throughput |
| PCIe Gen4 | 64 GB/s host bandwidth |

**MIG (Multi-Instance GPU):** Partition one A100 into up to 7 isolated instances. Key for inference and multi-tenant cloud.

---

## Historical significance

| Milestone | A100's role |
|-----------|-------------|
| GPT-3 (2020) | Trained on A100 clusters |
| DALL-E (2021) | A100 infrastructure |
| Stable Diffusion (2022) | A100 training |
| ChatGPT (2022) | A100 inference initially |
| AI boom (2020-2023) | Workhorse GPU |

The A100 era established NVIDIA's AI dominance before [[H100]] extended it.

---

## China variants

| Chip | Spec change | Purpose |
|------|-------------|---------|
| A800 | NVLink reduced to 400 GB/s | Export-compliant for China |
| A100 (original) | Banned Oct 2022 | [[Export controls]] |

A800 created specifically to comply with US export restrictions while preserving most capability.

---

## Successor comparison

| Spec | A100 80GB | H100 SXM | Improvement |
|------|-----------|----------|-------------|
| Transistors | 54B | 80B | 1.5x |
| HBM | 80GB HBM2e | 80GB HBM3 | Same capacity, faster |
| Bandwidth | 2 TB/s | 3.35 TB/s | 1.7x |
| FP8 | N/A | 3,958 TFLOPS | New capability |
| TDP | 400W | 700W | 1.75x |

---

## Current status

| Market | Status |
|--------|--------|
| New sales | Discontinued (replaced by H100/H200) |
| Cloud | Still available (AWS, GCP, Azure) |
| Used market | Active secondary market |
| China | A800 variant still shipping |
| Price (used) | ~$10,000-15,000 (was $15,000+ new) |

Legacy workloads and cost-sensitive deployments still use A100s.

---

## Related

- [[NVIDIA]] — parent actor
- [[AI accelerators]] — parent concept
- [[H100]] — successor (Hopper architecture, 2022)
- [[V100]] — predecessor (Volta architecture, 2017)
- [[HBM]] — memory technology
- [[TSMC]] — foundry partner
- [[Export controls]] — triggered A800 variant
