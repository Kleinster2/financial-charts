---
aliases:
  - SWE-Bench Verified
  - SWE-Bench Multilingual
  - SWEBench
tags:
  - concept
  - ai
  - benchmark
---

# SWE-Bench

Benchmark evaluating AI on real-world software engineering tasks — resolving GitHub issues from popular open-source Python repositories. Created by Princeton, launched 2023. SWE-Bench Verified is a human-validated subset; SWE-Bench Multilingual extends to non-Python languages.

---

## Why it matters

The standard benchmark for code generation and bug-fixing capability. Directly relevant to revenue-generating products like [[Claude Code]], [[GitHub Copilot]], and [[Cursor]]. Scores have risen rapidly — from ~12% (early 2024) to 70%+ (early 2026) — tracking the explosion in agentic coding tools.

---

## Current scores (Feb 2026)

| Model | SWE-Bench Verified |
|-------|--------------------|
| [[Moonshot AI]] | 76.8% |
| [[GLM]] 4.7 | 73.8% |

---

## Related

- [[AI benchmarks]] — parent concept
- [[Agentic AI]] — capability being measured
- [[Claude Code]] — leading agentic coding product
- [[Terminal-Bench]] — complementary CLI benchmark
