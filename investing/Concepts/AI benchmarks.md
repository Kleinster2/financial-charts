---
aliases:
  - AI benchmark
  - model benchmarks
  - LLM benchmarks
tags:
  - concept
  - ai
---

# AI benchmarks

Standardized evaluations for measuring AI model capabilities. Used by labs to differentiate products and by investors to track competitive positioning.

---

## Key benchmarks tracked

| Benchmark | Measures | Created by |
|-----------|----------|------------|
| [[OSWorld]] | Computer use / GUI automation | XLANG Lab (HKU) |
| [[Terminal-Bench]] | Agentic terminal/CLI tasks | Stanford / Laude Institute |
| [[Humanity's Last Exam]] | Expert-level multidisciplinary reasoning | CAIS / Scale AI |
| [[ARC-AGI]] | Abstract reasoning / fluid intelligence | [[Francois Chollet]] |
| [[SWE-Bench]] | Software engineering tasks | Princeton |
| [[MMLU]] | Academic knowledge breadth | UC Berkeley |

---

## Investment relevance

Benchmarks serve three functions in the vault:

1. Product tier differentiation — justifying pricing (e.g., [[Claude Opus]] vs [[Claude Sonnet]] on Terminal-Bench)
2. Competitive tracking — [[US-China AI race]], [[Open source commoditization]] use benchmark gaps as evidence
3. Capability validation — agentic benchmarks ([[OSWorld]], [[Terminal-Bench]]) directly correlate with product revenue potential (e.g., [[Claude Code]] at $2.5B run rate)

Gap closure is the key signal: [[MMLU]] gap between open and closed models shrank from 17.5 to 0.3 points in one year.

---

## Related

- [[Frontier models]] — primary subjects of benchmark evaluation
- [[Claude]] — [[Anthropic]]'s model family
- [[US-China AI race]] — benchmarks as competitive scoreboard
- [[Open source commoditization]] — benchmark convergence as evidence
