---
aliases:
  - MMLU benchmark
  - Massive Multitask Language Understanding
tags:
  - concept
  - ai
  - benchmark
---

# MMLU

Benchmark measuring broad academic knowledge across 57 subjects — STEM, humanities, social sciences, professional domains. Multiple-choice format. Created by UC Berkeley (Dan Hendrycks et al.), launched 2020.

---

## Why it matters

Was the standard general-knowledge benchmark from 2020-2024. Now largely saturated — frontier models score 90%+, making it less useful for differentiation. Primary significance in the vault is tracking open-source gap closure: the gap between open and closed models shrank from 17.5 to 0.3 points in one year, key evidence for [[Open source commoditization]].

Superseded by [[Humanity's Last Exam]] for frontier evaluation.

---

## Related

- [[AI benchmarks]] — parent concept
- [[Open source commoditization]] — MMLU gap as evidence
- [[Humanity's Last Exam]] — successor benchmark
- [[DeepSeek-V]] — notable open-source performance
