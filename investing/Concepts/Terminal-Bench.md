---
aliases:
  - Terminal-Bench 2.0
  - TerminalBench
tags:
  - concept
  - ai
  - benchmark
---

# Terminal-Bench

Benchmark for AI agents performing terminal/CLI tasks — compiling repositories, training ML models, debugging configurations, network setup, cybersecurity challenges. Tasks containerized in Docker with verification scripts. Created by Stanford and the Laude Institute, v1.0 launched May 2025, v2.0 followed with 89 validated tasks.

---

## Why it matters

As agentic coding tools ([[Claude Code]], [[Codex CLI]], [[Gemini CLI]]) become mainstream, Terminal-Bench measures whether they handle real professional terminal work. Frontier models score below 65%, below 50% on hardest tasks. The primary benchmark for [[Agentic AI]] coding products.

---

## Current scores (Feb 2026)

| Model | Score |
|-------|-------|
| [[Claude Opus]] 4.6 | 65.4% (#1) |
| [[Moonshot AI]] | 50.8% |

---

## Related

- [[AI benchmarks]] — parent concept
- [[Agentic AI]] — capability being measured
- [[Claude Code]] — leading product on this benchmark
- [[OSWorld]] — complementary benchmark (GUI vs CLI)
