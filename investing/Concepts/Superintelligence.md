---
aliases: [ASI, Artificial Superintelligence, AGI]
---
#concept #ai #frontier

**Superintelligence** — AI systems that exceed human cognitive abilities across virtually all domains. The stated long-term goal of multiple frontier AI labs. Also called Artificial Superintelligence (ASI) or, more modestly, Artificial General Intelligence (AGI).

---

## Definition spectrum

| Term | Definition | Timeframe estimates |
|------|------------|---------------------|
| **AGI** | AI matching human-level performance across tasks | 2-10 years (varies wildly) |
| **Superintelligence** | AI significantly exceeding human capability | Post-AGI |
| **Recursive self-improvement** | AI that can improve its own architecture | Theoretical |

**The key distinction**: AGI = human-level. Superintelligence = beyond human. Most labs publicly target AGI; some (like [[Recursive]]) explicitly target self-improving superintelligence.

---

## Labs pursuing superintelligence

| Lab | Valuation | Stated goal | Approach |
|-----|-----------|-------------|----------|
| [[OpenAI]] | ~$150B | AGI, then superintelligence | Scale + reasoning (o-series) |
| [[Anthropic]] | ~$60B | Safe AGI | Constitutional AI, interpretability |
| [[DeepMind]] | (Google) | AGI | Neuroscience-inspired, AlphaFold |
| [[xAI]] | ~$50B | "Truth-seeking" AI | Real-time data, Grok |
| [[Recursive]] | ~$4B (talks) | Self-improving superintelligence | Recursive improvement |
| [[SSI]] | ~$1B | Safe superintelligence | Founded by Ilya Sutskever |

---

## Recursive self-improvement

The specific thesis behind [[Recursive]] (Richard Socher's new lab):

**The idea**: Build AI that can improve itself without human feedback loops.

```
AI v1 → improves own code/architecture → AI v2 → improves further → AI v3...
```

**Why it matters**: If achieved, progress could accelerate dramatically — no longer bottlenecked by human researcher speed.

**Challenges**:
- Alignment: How do you ensure self-improving AI stays aligned with human values?
- Control: At what point can humans no longer understand or correct the system?
- Verification: How do you test something smarter than you?

---

## Safety concerns

The closer labs get to superintelligence, the more safety matters:

| Concern | Description |
|---------|-------------|
| **Alignment** | AI pursuing goals humans didn't intend |
| **Control** | Ability to correct/shut down advanced systems |
| **Value lock-in** | Early superintelligence "locking in" its values |
| **Concentration** | First-mover advantage too decisive |

**Lab positions**:
- [[Anthropic]]: Safety-first, interpretability research, Constitutional AI
- [[OpenAI]]: "Safe AGI" rhetoric, but aggressive shipping pace
- [[DeepMind]]: Academic safety research, slower deployment
- [[Recursive]]: Unknown — new lab, approach TBD

---

## Investment implications

**The race creates urgency**:
- Compute demand: Labs competing = everyone needs more GPUs
- Talent wars: Top researchers command $1M+ packages
- Capital intensity: $4B valuation for a lab with no product (Recursive)

**Uncertainty**:
- No one knows if superintelligence is 3 years or 30 years away
- Regulatory risk if breakthroughs trigger government intervention
- Winner-take-all dynamics possible (first superintelligence = decisive advantage?)

**Current investable exposure**:
- [[NVIDIA]] — picks and shovels, wins regardless of which lab
- [[Microsoft]] (OpenAI), [[Google]] (DeepMind), [[Amazon]] (Anthropic) — hyperscaler bets
- [[Anthropic]], [[OpenAI]] — private market, secondary sales

---

## Timeline debates

| View | Proponents | Estimate |
|------|------------|----------|
| **Imminent** | Some OpenAI researchers, Elon Musk | 2-5 years |
| **This decade** | Most frontier lab leadership | 5-10 years |
| **Uncertain** | Academic researchers | Could be decades |
| **Never** | Skeptics | Fundamental barriers |

**Sam Altman (OpenAI)**: "AGI is coming, and we're going to build it."
**Dario Amodei (Anthropic)**: Expects "powerful AI" by 2026-2027.
**Demis Hassabis (DeepMind)**: "A few years" to AGI (2023 comment).

---

## Related

### Labs
- [[OpenAI]] — largest, most aggressive
- [[Anthropic]] — safety-focused
- [[DeepMind]] — Google's lab
- [[xAI]] — Musk's lab
- [[Recursive]] — self-improving AI focus
- [[SSI]] — Ilya Sutskever's safe superintelligence startup

### People
- [[Sam Altman]] — OpenAI CEO
- [[Dario Amodei]] — Anthropic CEO
- [[Demis Hassabis]] — DeepMind CEO
- [[Ilya Sutskever]] — SSI founder, ex-OpenAI
- [[Richard Socher]] — Recursive founder

### Concepts
- [[Model lab economics]] — business model dynamics
- [[Model landscape]] — competitive positioning
- [[AI safety]] — alignment research
- [[OpenAI talent exodus]] — researcher departures

### Risks
- [[AI regulation]] — government intervention risk
- [[Export controls]] — compute access constraints
