---
aliases:
  - HLE
  - Humanity's Last Exam benchmark
tags:
  - concept
  - ai
  - benchmark
---

# Humanity's Last Exam

Expert-level multidisciplinary benchmark spanning ~100 academic subjects. 3,000 questions sourced from ~1,000 subject-matter experts across 500 institutions in 50 countries, filtered from 70,000+ candidates. Designed to be "Google-proof" — requiring genuine understanding, not retrieval. Created by Center for AI Safety (CAIS) and Scale AI, led by Dan Hendrycks. Launched January 2025.

---

## Why it matters

Created in response to benchmark saturation — [[MMLU]], GPQA, and other academic benchmarks were being solved by frontier models. Top AI scores are 35-37% vs human graduate students at ~90%, providing substantial headroom. Stanford AI Index 2025 cites it as a key challenging benchmark.

Limitation: independent review suggested ~30% of chemistry/biology answers may be incorrect.

---

## Current scores (Feb 2026)

| Model | Score |
|-------|-------|
| [[Claude Opus]] 4.6 | #1 |
| [[GLM]] 4.7 | 42.8% |

---

## Related

- [[AI benchmarks]] — parent concept
- [[Frontier models]] — primary subjects
- [[MMLU]] — predecessor (now saturated)
- [[US-China AI race]] — HLE as competitive scoreboard
