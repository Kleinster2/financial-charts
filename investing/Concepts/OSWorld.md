---
aliases:
  - OSWorld-Verified
  - OSWorld benchmark
tags:
  - concept
  - ai
  - benchmark
---

# OSWorld

Benchmark for evaluating AI agents on real computer tasks — clicking, typing, navigating GUIs across Ubuntu, Windows, and macOS. 369 tasks derived from actual computer use cases. Created by XLANG Lab (University of Hong Kong), launched April 2024. Published at NeurIPS 2024.

OSWorld-Verified (July 2025) improved task quality and infrastructure.

---

## Why it matters

First scalable benchmark using real (not simulated) computer environments. When launched, best model achieved 12.24% vs humans at 72.36%. By Feb 2026, frontier models match human-level performance (~72-73%).

Directly relevant to product revenue: [[Claude Cowork]] desktop app, [[Computer use]], and agent-driven workflows all depend on GUI automation capability.

---

## Current scores (Feb 2026)

| Model | Score |
|-------|-------|
| [[Claude Opus]] 4.6 | 72.7% |
| [[Claude Sonnet]] 4.6 | 72.5% |
| Human baseline | 72.36% |

---

## Related

- [[AI benchmarks]] — parent concept
- [[Computer use]] — capability being measured
- [[Claude Cowork]] — desktop agent product
- [[Terminal-Bench]] — complementary benchmark (CLI vs GUI)
