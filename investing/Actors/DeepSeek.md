---
aliases: [DeepSeek AI, DeepSeek R1]
---
#actor #china #ai #models

**DeepSeek** — Hangzhou-based AI lab. DeepSeek R1 model notable for efficiency. Available on Azure, competitive with frontier models at lower cost.

---

## Why DeepSeek matters

China's most prominent open-ish AI lab, known for efficiency:

| Metric | Value |
|--------|-------|
| HQ | [[Hangzhou]] |
| Key model | DeepSeek R1 |
| Approach | Efficiency-focused, open weights |
| Availability | Azure, Hugging Face, self-host |
| Backed by | High-Flyer (quant fund) |

---

## DeepSeek R1

| Spec | Details |
|------|---------|
| Architecture | Mixture of Experts (MoE) |
| Efficiency | Lower cost/token than GPT-4 |
| Performance | Competitive on benchmarks |
| Weights | Open (downloadable) |
| Inference | Runs on consumer hardware |

**Why it matters:** Proves China can build competitive models despite GPU restrictions. Efficiency offsets hardware disadvantage.

---

## Available on major platforms

| Platform | Status |
|----------|--------|
| [[Microsoft]] Azure | ✓ (listed alongside GPT, Claude) |
| Hugging Face | ✓ (open weights) |
| Self-hosted | ✓ |
| China cloud | ✓ (Alibaba, etc.) |

Azure listing is notable — Microsoft offering Chinese model alongside OpenAI, Anthropic.

---

## Backing: High-Flyer

DeepSeek is backed by High-Flyer (幻方量化), a major Chinese quant fund:

| Aspect | Details |
|--------|---------|
| Type | Quantitative hedge fund |
| AUM | $8B+ |
| Why AI | Compute for trading → AI research |
| Approach | Patient capital, long-term research |

**Unusual model:** Quant fund funding AI lab. Compute infrastructure repurposed.

---

## Efficiency thesis

DeepSeek represents China's response to GPU constraints:

| US approach | China/DeepSeek approach |
|-------------|-------------------------|
| Best chips (Blackwell) | Efficient algorithms |
| Scale compute | Optimize per-FLOP |
| Frontier capability | Competitive at lower cost |

**The insight:** If you can't get the best chips, make better use of the chips you have. MoE architecture, distillation, quantization.

---

## Competitive positioning

| vs Model | DeepSeek advantage | Disadvantage |
|----------|-------------------|--------------|
| GPT-4 | Cheaper, open weights | Less capable on some tasks |
| Claude | Self-hostable | Smaller context, less polish |
| Llama | Comparable openness | Less community adoption |
| Qwen | More efficient | Alibaba has more resources |

---

## Financials

| Metric | Value |
|--------|-------|
| Status | Private (not separately funded) |
| Backer | High-Flyer (幻方量化) |
| High-Flyer AUM | $8B+ |
| Valuation | Not disclosed (internal project) |
| Revenue model | API access, open weights |
| Funding approach | Internal (quant fund profits) |

**Unusual structure:** DeepSeek is not a typical startup. It's an AI research arm of High-Flyer, funded by quant trading profits. No external VC rounds announced.

---

## Investment implications

**Private company** — not directly investable.

**Indirect exposure:**
- [[Microsoft]] — offers R1 on Azure
- [[Alibaba]] — offers on China cloud
- [[NVIDIA]] — still needs GPUs (H200 likely)

**Thesis implications:**
- Validates China AI capability despite [[Export controls]]
- Efficiency-focused approach may influence global model development
- Open weights pressure closed model pricing

---

*Updated 2026-01-03*

---

## Related

- [[Hangzhou]] — HQ (China's AI hub)
- [[Microsoft]] — distribution (Azure)
- [[Model landscape]] — context (China open models)
- [[China AI clusters]] — context (compute infrastructure)
- [[Export controls]] — constraint (GPU restrictions)
- [[Alibaba]] — peer (Qwen models)
- [[ByteDance]] — peer (Doubao)
