Machine learning researcher and educator. Author of influential from-scratch AI books.

## Books

- **Build a Large Language Model from Scratch** — implements GPT-2 architecture, teaches fundamentals by building
- **Build a Reasoning Model from Scratch** — extends to reasoning/RL techniques

Philosophy: "If there is code, and the code works, you know it's correct. There's no misunderstanding. It's precise."

## Background

Previously academic researcher in computational biology. Maintains mlxtend, a popular ML library. Active on YouTube with courses.

## Views

On open models: Chinese open-weight models popular partly because licenses are unrestricted — no strings attached unlike [[Llama]] or [[Gemma]] which have user limits and reporting requirements.

On AI progress: Architecture hasn't fundamentally changed from GPT-2. Gains come from post-training (RLVR, RLHF), data quality, and inference scaling — not new architectures.

## Related

- [[Nathan Lambert]]
- [[ATOM Project]]
- [[Allen Institute for AI]]
