---
aliases: []
---
#actor #ai #semiconductor #usa #private

**Groq** â€” LPU inference chips. Fastest tokens/sec. Founded by ex-Google TPU architect.

---

## Why Groq matters

Inference speed leader:

| Metric | Value |
|--------|-------|
| Valuation | ~$2.8B (2024) |
| Raised | $640M+ |
| Founded | 2016 |
| Founder | Jonathan Ross (Google TPU) |

---

## Language Processing Unit (LPU)

**Inference-optimized architecture:**
- Deterministic execution
- No HBM dependency (SRAM-based)
- Predictable latency
- 500+ tokens/sec on Llama

---

## Speed advantage

| Metric | Groq | GPU comparison |
|--------|------|----------------|
| Tokens/sec | 500+ | ~50-100 |
| Latency | Ultra-low | Variable |
| Power efficiency | Better | Higher power |

10x+ faster inference than GPUs.

---

## GroqCloud

**Inference API service:**
- Pay-per-token pricing
- Fastest API available
- Open models (Llama, Mixtral)
- Developer-friendly

---

## Business model

**Cloud-first:**
- GroqCloud API (primary)
- On-prem systems
- Inference-as-a-service
- Not selling chips directly

---

## Technical approach

**Tensor Streaming Processor (TSP):**
- Deterministic dataflow
- Compiler does scheduling
- No runtime decisions
- Predictable performance

---

## Limitations

**Trade-offs:**
- Inference only (no training)
- Smaller models preferred
- SRAM = less memory capacity
- New ecosystem

---

## Investment case

**Bull:**
- Fastest inference speeds
- TPU architect pedigree
- Inference market growing
- Differentiated architecture

**Bear:**
- NVIDIA CUDA ecosystem
- Training still on GPUs
- Memory capacity limits
- Scaling challenges

---

## Quick stats

| Metric | Value |
|--------|-------|
| Ticker | Private |
| Valuation | ~$2.8B |
| Approach | LPU inference chips |
| Founder | Jonathan Ross (ex-Google TPU) |

*Updated 2026-01-01*

---

Related: [[NVIDIA]], [[Cerebras]], [[SambaNova]], [[Google]]

