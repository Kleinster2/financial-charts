---
aliases: [Flapping Airplanes]
tags: [actor, ai, startup, research]
---

#actor #ai #startup #research

**Flapping Airplanes** — research-first AI lab focused on **data efficiency** — finding less data-hungry ways to train AI. Betting that current frontier models (trained on all human knowledge) are fundamentally wasteful, and brain-inspired approaches can be orders of magnitude more efficient.

---

## Quick stats

| Metric | Value |
|--------|-------|
| Founded | Jan 2026 |
| HQ | — |
| Founders | Ben Spector, Asher Spector (brothers), Aidan Smith (ex-[[Neuralink]]) |
| Stage | Seed |
| Funding | $180M seed |

---

## Funding

| Round | Date | Amount | Investors |
|-------|------|--------|-----------|
| **Seed** | **Jan 29, 2026** | **$180M** | **[[GV]]** (Google Ventures), **[[Sequoia Capital]]**, **[[Index Ventures]]** |

$180M seed is extraordinary — signals top-tier VC conviction in the thesis.

---

## The thesis

**Name origin:** Early aviation pioneers tried to build planes that flapped like birds — they failed. Flight was achieved through different principles (fixed wings, aerodynamics). The question for AI: are current approaches (transformers, gradient descent, brute-force scaling) the equivalent of "flapping"? Is there a fundamentally different path?

**Core bet (three parts):**
1. **Data efficiency is the key problem** — humans learn from vastly less data than LLMs. A million-times more data-efficient model would transform AI economics
2. **Brain-inspired algorithms** — human learning uses fundamentally different mechanisms than gradient descent. Understanding that gap unlocks new architectures
3. **Inexperience as advantage** — a young, creative team looking at problems from the ground up, unburdened by existing assumptions

**Ben Spector:** "The current frontier models are trained on the sum totality of human knowledge, and humans can obviously make do with an awful lot less. So there's a big gap."

**Aidan Smith:** "When you look inside the brain, the algorithms it uses are just fundamentally so different from gradient descent."

---

## Why it matters

If data-efficient training works, it disrupts the entire AI scaling paradigm:
- **Robotics** — currently data-constrained, can't learn from internet text
- **Scientific discovery** — domain data is scarce
- **Enterprise** — a model 1M times more data-efficient is 1M times easier to deploy
- **Compute economics** — less data = less compute = cheaper = [[DeepSeek]]-like disruption of scaling maximalism

This directly challenges the Stargate/hyperscale thesis. If training efficiency improves drastically, the $500B data center buildouts may be overbuilt.

---

## Related

- [[DeepSeek]] — parallel thesis (efficiency beats scale)
- [[Anthropic]] — "real DeepSeek" efficiency angle (see [[Long Anthropic]])
- [[Neuralink]] — Aidan Smith's background (brain-computer interface)
- [[GV]] — seed investor (Google Ventures)
- [[Sequoia Capital]] — seed investor
- [[Index Ventures]] — seed investor
- [[AI capex arms race]] — counter-thesis (if efficiency wins, capex overspend)

*Source: TechCrunch (Jan 29 + Feb 16, 2026)*
*Created 2026-02-18*
